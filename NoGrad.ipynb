{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This note shows how to use torch.no_grad(), .requires_grad, .detach() by case study.\n",
    "To sum up, torch.no_grad() affects the output variables while keeping paramters of the model unchanged; \n",
    ".requires_grad could directly affect if paramters require grad or not, however, it doesn't affect the output variables\n",
    "of the model;\n",
    ".detach() is used to copy one or several specific variables to other variables with .requires_grad = False without \n",
    "affecting the whole graph.\n",
    "\n",
    "References: https://pytorch.org/docs/master/notes/autograd.html\n",
    "https://discuss.pytorch.org/t/clone-and-detach-in-v0-4-0/16861\n",
    "https://stackoverflow.com/questions/51748138/pytorch-how-to-set-requires-grad-false\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1278, -2.2626],\n",
       "        [ 0.2183, -1.5492]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By default, a variable.requires_grad is False\n",
    "x1 = torch.randn(2, 2)\n",
    "print(x1.requires_grad)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1053, -1.5058],\n",
       "        [ 0.1517, -1.2914]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a variabe with .requires_grad True\n",
    "x2 = torch.randn(2, 2)\n",
    "x2.requires_grad = True\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_backend': <torch.nn.backends.thnn.THNNFunctionBackend at 0x109d87390>,\n",
       " '_parameters': OrderedDict([('weight', Parameter containing:\n",
       "               tensor([[ 0.4102, -0.0866],\n",
       "                       [ 0.2055,  0.6210]], requires_grad=True)),\n",
       "              ('bias', Parameter containing:\n",
       "               tensor([-0.2167,  0.4468], requires_grad=True))]),\n",
       " '_buffers': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict(),\n",
       " 'training': True,\n",
       " 'in_features': 2,\n",
       " 'out_features': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define several models. by default their parameters have .requires_grad=Truw\n",
    "lin0 = nn.Linear(2, 2)\n",
    "lin1 = nn.Linear(2, 2)\n",
    "lin2 = nn.Linear(2, 2)\n",
    "vars(lin0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8470, -1.8848],\n",
      "        [ 0.5216, -1.1608]]) tensor([[ 0.3245, -0.5851],\n",
      "        [ 0.3245, -0.5851]]) None\n"
     ]
    }
   ],
   "source": [
    "x3 = lin0(x2)\n",
    "x4 = lin1(x3)\n",
    "x4.sum().backward()\n",
    "print(lin0.weight.grad, lin1.weight.grad, lin2.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Use torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    x5 = lin2(x4)\n",
    "    print(x4.requires_grad)\n",
    "    print(lin2.weight.requires_grad)\n",
    "    print(x5.requires_grad)\n",
    "# The result shows that the input variable x4 is not changed, the parameters of model lin2 still have \n",
    "#.requires_grad=True. However, the output variable x5 has .requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None tensor([[-0.3956, -1.6145],\n",
      "        [-0.3956, -1.6145]])\n"
     ]
    }
   ],
   "source": [
    "# Notice that once the gradient tracking is cut-off by torch.no_grad(), you won't be able to backpropagate the \n",
    "# gradient to layers before the no_grad.\n",
    "lin0 = nn.Linear(2, 2)\n",
    "lin1 = nn.Linear(2, 2)\n",
    "lin2 = nn.Linear(2, 2)\n",
    "x3 = lin0(x2)\n",
    "with torch.no_grad():\n",
    "    x4 = lin1(x3)\n",
    "x5 = lin2(x4)\n",
    "x5.sum().backward()\n",
    "print(lin0.weight.grad, lin1.weight.grad, lin2.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_backend': <torch.nn.backends.thnn.THNNFunctionBackend object at 0x109d87390>, '_parameters': OrderedDict([('weight', Parameter containing:\n",
      "tensor([[-0.3327, -0.0667],\n",
      "        [-0.2523,  0.0855]])), ('bias', Parameter containing:\n",
      "tensor([ 0.1044, -0.6160]))]), '_buffers': OrderedDict(), '_backward_hooks': OrderedDict(), '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'training': True, 'in_features': 2, 'out_features': 2}\n"
     ]
    }
   ],
   "source": [
    "# Use .requires_grad to freeze some parameters of model. This is a very useful case if you want to use the output of\n",
    "# a trained model as features for downstream task.\n",
    "for param in lin1.parameters():\n",
    "    param.requires_grad = False\n",
    "print(vars(lin1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7582,  1.6871],\n",
      "        [ 0.4078, -0.9074]]) None tensor([[-0.2020,  0.2053],\n",
      "        [-0.2020,  0.2053]])\n"
     ]
    }
   ],
   "source": [
    "# The output variable of a frozen model keeps the gradient tracking and you are able to backpropogate through it. \n",
    "# However, it doesn't affect the gradient. This is a good proprety that we want.\n",
    "lin0 = nn.Linear(2, 2)\n",
    "lin1 = nn.Linear(2, 2)\n",
    "lin2 = nn.Linear(2, 2)\n",
    "x3 = lin0(x2)\n",
    "for param in lin1.parameters():\n",
    "    param.requires_grad = False\n",
    "x4 = lin1(x3)\n",
    "x5 = lin2(x4)\n",
    "x5.sum().backward()\n",
    "print(lin0.weight.grad, lin1.weight.grad, lin2.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5972, 1.3182],\n",
      "        [0.4222, 0.8155]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.5972, 1.3182],\n",
      "        [0.4222, 0.8155]], grad_fn=<ThresholdBackward0>)\n",
      "tensor([[-0.6268,  0.4045],\n",
      "        [-0.5410,  0.3039]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.5108, -0.0270],\n",
      "        [-0.4217, -0.0467]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "lin0 = nn.Linear(2, 2)\n",
    "nonlin0 = nn.ReLU()\n",
    "lin1 = nn.Linear(2, 2)\n",
    "lin2 = nn.Linear(2, 2)\n",
    "x2 = lin0(x1)\n",
    "x3 = nonlin0(x2)\n",
    "for param in lin1.parameters():\n",
    "    param.requires_grad = False\n",
    "x4 = lin1(x3)\n",
    "x5 = lin2(x4)\n",
    "print(x2)\n",
    "print(x3)\n",
    "print(x4)\n",
    "print(x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Since lin1 .requires_grad = False, if the input variable doesn't require grad, then the output neither; \n",
    "# otherwise, the output has requires_grad=True\n",
    "x0 = torch.randn(2,2)\n",
    "x1= lin1(x0)\n",
    "x2 = lin2(x0)\n",
    "print(x0.requires_grad)\n",
    "print(x1.requires_grad)\n",
    "print(x2.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3828, -0.1423],\n",
      "        [ 0.4588, -0.2701]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.3828, -0.1423],\n",
      "        [ 0.4588, -0.2701]])\n",
      "tensor([[-0.5769, -0.3604],\n",
      "        [-0.6140, -0.2757]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Use .detach() to get x2 without grad and the whole computing graph stays same.\n",
    "lin0 = nn.Linear(2, 2)\n",
    "lin1 = nn.Linear(2, 2)\n",
    "x1 = torch.randn(2, 2)\n",
    "x2 = lin0(x1)\n",
    "x3 = lin1(x2)\n",
    "output = x2.detach()\n",
    "print(x2)\n",
    "print(output)\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None tensor([[ 0.8416, -0.4124],\n",
      "        [ 0.8416, -0.4124]])\n"
     ]
    }
   ],
   "source": [
    "# You can use output as part of other graphs.\n",
    "x4 = lin1(output)\n",
    "x4.sum().backward()\n",
    "print(lin0.weight.grad, lin1.weight.grad)\n",
    "# The backward of this new graph doesn't affect the initial graph. That's why lin0.weight.grad is None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
